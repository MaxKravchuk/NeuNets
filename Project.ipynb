{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {},
   "source": [
    "# Project: Sign Language Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import ResNet50, ResNet50V2, ResNet101V2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, LSTM, Reshape, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import splitfolders\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {},
   "source": [
    "### Loading the ASL dataset and splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"debashishsau/aslamerican-sign-language-aplhabet-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path where our data is located\n",
    "base_path = Path(path) / \"ASL_Alphabet_Dataset\" / \"asl_alphabet_train\"\n",
    "\n",
    "# Dictionary to save our 29 classes\n",
    "categories = {  0: \"A\",\n",
    "                1: \"B\",\n",
    "                2: \"C\",\n",
    "                3: \"D\",\n",
    "                4: \"E\",\n",
    "                5: \"F\",\n",
    "                6: \"G\",\n",
    "                7: \"H\",\n",
    "                8: \"I\",\n",
    "                9: \"G\",\n",
    "                10: \"K\",\n",
    "                11: \"L\",\n",
    "                12: \"M\",\n",
    "                13: \"N\",\n",
    "                14: \"O\",\n",
    "                15: \"P\",\n",
    "                16: \"Q\",\n",
    "                17: \"R\",\n",
    "                18: \"S\",\n",
    "                19: \"T\",\n",
    "                20: \"U\",\n",
    "                21: \"V\",\n",
    "                22: \"W\",\n",
    "                23: \"X\",\n",
    "                24: \"Y\",\n",
    "                25: \"Z\",\n",
    "                26: \"del\",\n",
    "                27: \"nothing\",\n",
    "                28: \"space\",\n",
    "            }\n",
    "\n",
    "def add_class_name_prefix(df, col_name):\n",
    "    df[col_name] = df[col_name].apply(lambda x: f\"class_{x}\")\n",
    "    return df\n",
    "\n",
    "# List containing all the filenames in the dataset\n",
    "filenames_list = []\n",
    "# List to store the corresponding category; note that each folder of the dataset has one class of data\n",
    "categories_list = []\n",
    "\n",
    "for category_id, category_name in categories.items():\n",
    "    category_path = base_path / category_name\n",
    "    if category_path.exists() and category_path.is_dir():\n",
    "        filenames = [file.name for file in category_path.iterdir() if file.is_file()]\n",
    "        filenames_list.extend(filenames)\n",
    "        categories_list.extend([category_id] * len(filenames))\n",
    "\n",
    "df = pd.DataFrame({\"filename\": filenames_list, \"category\": categories_list})\n",
    "df = add_class_name_prefix(df, \"filename\")\n",
    "\n",
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path('./asl_dataset_split')\n",
    "if not output_path.exists():\n",
    "    splitfolders.ratio(base_path, output=output_path, seed=1333, ratio=(0.8, 0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explicit paths for train, val, and test\n",
    "train_path = output_path / 'train'\n",
    "val_path = output_path / 'val'\n",
    "test_path = output_path / 'test'\n",
    "\n",
    "print(f\"Train data saved at: {train_path}\")\n",
    "print(f\"Validation data saved at: {val_path}\")\n",
    "print(f\"Test data saved at: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Parameters\n",
    "image_size = 64  # resizing images to 64x64\n",
    "batch_size = 32  \n",
    "\n",
    "# data augmentation and normalization\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,  # normalize pixel values to [0, 1]\n",
    "    validation_split=0.2  # split data into training and validation\n",
    ")\n",
    "\n",
    "# training data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# validation data\n",
    "val_data = datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_data = datagen.flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Verifying shapes\n",
    "print(f\"Number of training samples: {train_data.samples}\")\n",
    "print(f\"Number of validation samples: {val_data.samples}\")\n",
    "print(f\"Number of test samples: {test_data.samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {},
   "source": [
    "## Load pretrained ResNet-50, building rnn, cnn and hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pretrained ResNet-50\n",
    "\n",
    "def load_resnet_base(input_shape=(64, 64, 3)):\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    return base_model\n",
    "\n",
    "resnet_base_model = load_resnet_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_with_rnn(base_model, rnn_layers=2):\n",
    "    base_model.trainable = False # we are freezing the base_model\n",
    "    inputs = Input(shape=(64, 64, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1, -1))(x)\n",
    "    for _ in range(rnn_layers):\n",
    "        x = LSTM(64, return_sequences=True)(x)\n",
    "    x = LSTM(64)(x)\n",
    "    outputs = Dense(29, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "resnet_rnn_model = resnet_with_rnn(resnet_base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {},
   "source": [
    "## Building the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_with_cnn(base_model, cnn_layers=2):\n",
    "    base_model.trainable = False # we are freezing the base_model\n",
    "    inputs = Input(shape=(64, 64, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    for _ in range(cnn_layers - 1):\n",
    "        x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(29, activation='softmax')(x)  # Updated for 29 classes\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "resnet_cnn_model = resnet_with_cnn(resnet_base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_resnet_cnn_rnn(base_model):\n",
    "    base_model.trainable = False\n",
    "    inputs = Input(shape=(64, 64, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1, -1))(x)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = LSTM(64)(x)\n",
    "    outputs = Dense(29, activation='softmax')(x)  # Updated for 29 classes\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "hybrid_model = hybrid_resnet_cnn_rnn(resnet_base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {},
   "source": [
    "## Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def evaluate_model(model, train_data, val_data, test_data, model_name, results, model_filename):\n",
    "    weights_dir = Path(mo.notebook_dir()) / \"weights\"\n",
    "    # Define file paths based on model_name\n",
    "    weights_file = weights_dir / f\"{model_filename}.weights.h5\"\n",
    "    history_file = weights_dir / f\"{model_filename}_history.json\"\n",
    "\n",
    "    # Check if both weights and history files exist\n",
    "    if weights_file.exists() and history_file.exists():\n",
    "        print(f\"Weights file found: {weights_file}. Loading weights...\")\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "        print(f\"History file found: {history_file}. Loading history...\")\n",
    "\n",
    "        with history_file.open('r') as f:\n",
    "            history = SimpleNamespace(history = json.load(f))\n",
    "\n",
    "        training_time = \"-\"\n",
    "    else:\n",
    "\n",
    "        # Early stopping callback to avoid overfitting\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            train_data,\n",
    "            validation_data=val_data,\n",
    "            epochs=30,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "    # Evaluate on test data\n",
    "    test_loss, test_accuracy = model.evaluate(test_data, verbose=0)\n",
    "\n",
    "    # Log results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Validation Accuracy': max(history.history['val_accuracy']),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Training Time (s)': training_time\n",
    "    })\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} Training/Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {},
   "source": [
    "### Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# evaluate all models\n",
    "history_resnet_rnn = evaluate_model(resnet_rnn_model, train_data, val_data, test_data, \"ResNet50 + RNN\", results, \"r50_rnn\")\n",
    "history_resnet_cnn = evaluate_model(resnet_cnn_model, train_data, val_data, test_data, \"ResNet50 + CNN\", results, \"r50_cnn\")\n",
    "history_hybrid = evaluate_model(hybrid_model, train_data, val_data, test_data, \"Hybrid ResNet50 (CNN + RNN)\", results, \"r50_hybrid\")\n",
    "\n",
    "# convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# display the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfG",
   "metadata": {},
   "source": [
    "### Resnetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_base_model = ResNet50V2(include_top=False, weights='imagenet', input_shape=(64, 64, 3))\n",
    "\n",
    "v2_rnn_model = resnet_with_rnn(v2_base_model)\n",
    "v2_cnn_model = resnet_with_cnn(v2_base_model)\n",
    "v2_hybrid = hybrid_resnet_cnn_rnn(v2_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_results = []\n",
    "\n",
    "# evaluate all models\n",
    "history_r50v2_rnn = evaluate_model(v2_rnn_model, train_data, val_data, test_data, \"ResNet50v2 + RNN\", v2_results, \"r50v2_rnn\")\n",
    "history_r50v2_cnn = evaluate_model(v2_cnn_model, train_data, val_data, test_data, \"ResNet50v2 + CNN\", v2_results, \"r50v2_cnn\")\n",
    "history_r50v2_hybrid = evaluate_model(v2_hybrid, train_data, val_data, test_data, \"Hybrid ResNet50v2 (CNN + RNN)\", v2_results, \"r50v2_hybrid\")\n",
    "\n",
    "# convert results to DataFrame\n",
    "v2_results_df = pd.DataFrame(v2_results)\n",
    "\n",
    "# display the results\n",
    "v2_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aLJB",
   "metadata": {},
   "source": [
    "### Resnet101v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "r101_base_model = ResNet101V2(include_top=False, weights='imagenet', input_shape=(64, 64, 3))\n",
    "\n",
    "r101_rnn_model = resnet_with_rnn(r101_base_model)\n",
    "r101_cnn_model = resnet_with_cnn(r101_base_model)\n",
    "r101_hybrid = hybrid_resnet_cnn_rnn(r101_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "r101_results = []\n",
    "\n",
    "# evaluate all models\n",
    "history_r101v2_rnn = evaluate_model(r101_rnn_model, train_data, val_data, test_data, \"ResNet101v2 + RNN\", r101_results, \"r101v2_rnn\")\n",
    "history_r101v2_cnn = evaluate_model(r101_cnn_model, train_data, val_data, test_data, \"ResNet101v2 + CNN\", r101_results, \"r101v2_cnn\")\n",
    "history_r101v2_hybrid = evaluate_model(r101_hybrid, train_data, val_data, test_data, \"Hybrid ResNet101v2 (CNN + RNN)\", r101_results, \"r101v2_hybrid\")\n",
    "\n",
    "# convert results to DataFrame\n",
    "r101_results_df = pd.DataFrame(r101_results)\n",
    "\n",
    "# display the results\n",
    "r101_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AjVT",
   "metadata": {},
   "source": [
    "## Saving Select Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = Path(mo.notebook_dir()) / \"weights\"\n",
    "weights_dir.mkdir(exist_ok=True)\n",
    "\n",
    "resnet_rnn_model.save_weights(weights_dir / \"r50_rnn.weights.h5\")\n",
    "resnet_cnn_model.save_weights(weights_dir / \"r50_cnn.weights.h5\")\n",
    "hybrid_model.save_weights(weights_dir / \"r50_hybrid.weights.h5\")\n",
    "\n",
    "json.dump(history_resnet_rnn.history, open(weights_dir / \"r50_rnn_history.json\", 'w' ))\n",
    "json.dump(history_resnet_cnn.history, open(weights_dir / \"r50_cnn_history.json\", 'w' ))\n",
    "json.dump(history_hybrid.history, open(weights_dir / \"r50_hybrid_history.json\", 'w' ))\n",
    "\n",
    "v2_rnn_model.save_weights(weights_dir / \"r50v2_rnn.weights.h5\")\n",
    "v2_cnn_model.save_weights(weights_dir / \"r50v2_cnn.weights.h5\")\n",
    "v2_hybrid.save_weights(weights_dir / \"r50v2_hybrid.weights.h5\")\n",
    "\n",
    "json.dump(history_r50v2_rnn.history, open(weights_dir / \"r50v2_rnn_history.json\", 'w' ))\n",
    "json.dump(history_r50v2_cnn.history, open(weights_dir / \"r50v2_cnn_history.json\", 'w' ))\n",
    "json.dump(history_r50v2_hybrid.history, open(weights_dir / \"r50v2_hybrid_history.json\", 'w' ))\n",
    "\n",
    "r101_rnn_model.save_weights(weights_dir / \"r101v2_rnn.weights.h5\")\n",
    "r101_cnn_model.save_weights(weights_dir / \"r101v2_cnn.weights.h5\")\n",
    "r101_hybrid.save_weights(weights_dir / \"r101v2_hybrid.weights.h5\")\n",
    "\n",
    "json.dump(history_r101v2_rnn.history, open(weights_dir / \"r101v2_rnn_history.json\", 'w' ))\n",
    "json.dump(history_r101v2_cnn.history, open(weights_dir / \"r101v2_cnn_history.json\", 'w' ))\n",
    "json.dump(history_r101v2_hybrid.history, open(weights_dir / \"r101v2_hybrid_history.json\", 'w' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NCOB",
   "metadata": {},
   "source": [
    "### Loading Weights Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqbW",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "test_model = resnet_with_cnn(r101_base_model)\n",
    "\n",
    "# Restore the weights\n",
    "test_model.load_weights(weights_dir / \"r101v2_cnn.weights.h5\")\n",
    "\n",
    "print(test_model.evaluate(test_data, verbose=0))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
